{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15a28227-f8db-470a-a6b3-ccc9da7f97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3fba4c9-34fd-4571-9e20-febab1814c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.10 (default, Jan  4 2024, 11:59:19) \\n[GCC 11.4.0]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63219337-05d0-442b-b742-058a01a3efcf",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36f72a56-b158-45f3-bd75-33b7a2ee4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f723e48-aa1b-4d04-b576-f237cb8b00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set at program startup\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4af43fe4-91e1-4019-a545-998ef4a587cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25c806e4-00f3-4aa4-a9df-5c9b4f84c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model_spec.get('efficientdet_lite1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebaefc31-66b4-4b89-a576-a36bd8537c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader.DataLoader"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_detector.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e574a12d-ecfb-4dac-b9ce-7b9907b50050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mobject_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pascal_voc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mimages_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mannotations_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabel_map\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mannotation_filenames\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_difficult_instances\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_shards\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_num_images\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_prefix_filename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mDetectorDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Loads from dataset with PASCAL VOC format.\n",
       "\n",
       "Refer to\n",
       "https://towardsdatascience.com/coco-data-format-for-object-detection-a4c5eaf518c5\n",
       "for the description of PASCAL VOC data format.\n",
       "\n",
       "LabelImg Tool (https://github.com/tzutalin/labelImg) can annotate the image\n",
       "and save annotations as XML files in PASCAL VOC data format.\n",
       "\n",
       "Annotations are in the folder: `annotations_dir`.\n",
       "Raw images are in the foloder: `images_dir`.\n",
       "\n",
       "Args:\n",
       "  images_dir: Path to directory that store raw images.\n",
       "  annotations_dir: Path to the annotations directory.\n",
       "  label_map: Variable shows mapping label integers ids to string label\n",
       "    names. 0 is the reserved key for `background`. Label names can't be\n",
       "    duplicated. Supported format: 1. Dict, map label integers ids to string\n",
       "      label names, e.g.\n",
       "       {1: 'person', 2: 'notperson'}. 2. List, a list of label names. e.g.\n",
       "         ['person', 'notperson'] which is\n",
       "       the same as setting label_map={1: 'person', 2: 'notperson'}.\n",
       "    3. String, name for certain dataset. Accepted values are: 'coco', 'voc'\n",
       "      and 'waymo'. 4. String, yaml filename that stores label_map.\n",
       "  annotation_filenames: Collection of annotation filenames (strings) to be\n",
       "    loaded. For instance, if there're 3 annotation files [0.xml, 1.xml,\n",
       "    2.xml] in `annotations_dir`, setting annotation_filenames=['0', '1']\n",
       "    makes this method only load [0.xml, 1.xml].\n",
       "  ignore_difficult_instances: Whether to ignore difficult instances.\n",
       "    `difficult` can be set inside `object` item in the annotation xml file.\n",
       "  num_shards: Number of shards for output file.\n",
       "  max_num_images: Max number of imags to process.\n",
       "  cache_dir: The cache directory to save TFRecord, metadata and json file.\n",
       "    When cache_dir is not set, a temporary folder will be created and will\n",
       "    not be removed automatically after training which makes it can be used\n",
       "    later.\n",
       "  cache_prefix_filename: The cache prefix filename. If not set, will\n",
       "    automatically generate it based on `image_dir`, `annotations_dir` and\n",
       "    `annotation_filenames`.\n",
       "\n",
       "Returns:\n",
       "  ObjectDetectorDataLoader object.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/data_util/object_detector_dataloader.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object_detector.DataLoader.from_pascal_voc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38659677-20df-4677-b4bc-13c4e75e376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_detector.DataLoader.from_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8144a-ba97-4aae-a7f0-c0a4fade963f",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98afdbea-3f03-4c3e-b5c0-24a34e4b2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc02f685-fa36-4872-8951-d62e5f2cb92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/technonatura-ai/Desktop/R2045/CenterStage-WorldChampionship-TFLite'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58caba06-5615-429c-a5a9-f9518df17d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = object_detector.DataLoader.from_pascal_voc(f\"{os.getcwd()}/PascalVOC-CenterCrop/train/imgs\", f\"{os.getcwd()}/PascalVOC-CenterCrop/train/annotations\",[\"teamprop_red\", \"teamprop_blue\"])\n",
    "validation_data  = object_detector.DataLoader.from_pascal_voc(f\"{os.getcwd()}/PascalVOC-CenterCrop/valid/imgs\", f\"{os.getcwd()}/PascalVOC-CenterCrop/valid/annotations\", [ \"teamprop_red\", \"teamprop_blue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ba286a7-ffe5-44c5-b8b8-9f24a6991477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'teamprop_red', 2: 'teamprop_blue'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b86c8f4-09a1-460b-9606-235d3ea4ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = object_detector.DataLoader.from_pascal_voc(f\"{os.getcwd()}/PascalVOC-CenterCrop/test/imgs\", f\"{os.getcwd()}/PascalVOC-CenterCrop/test/annotations\", [ \"teamprop_red\", \"teamprop_blue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "332b4b2b-627e-4841-a4ab-9c6766cb8f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1a57c95-8cfb-4c59-902f-b3f743459d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2eddbe8-434b-45c4-8a3b-017014db9c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e5c20-461d-4a46-a31e-e5671cba0f8e",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3491946-7c35-43cc-91d1-cbfb8ca1a874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee508330-1d6c-4d55-b238-bf21d1c3aba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:12:22.257771: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - ETA: 0s - det_loss: 0.6473 - cls_loss: 0.3804 - box_loss: 0.0053 - reg_l2_loss: 0.0634 - loss: 0.7107 - learning_rate: 0.0053 - gradient_norm: 4.6021"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:13:12.643356: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 53s 113ms/step - det_loss: 0.6463 - cls_loss: 0.3799 - box_loss: 0.0053 - reg_l2_loss: 0.0634 - loss: 0.7096 - learning_rate: 0.0053 - gradient_norm: 4.5976 - val_det_loss: 0.2302 - val_cls_loss: 0.1616 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.0636 - val_loss: 0.2938\n",
      "Epoch 2/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.3040 - cls_loss: 0.1902 - box_loss: 0.0023 - reg_l2_loss: 0.0636 - loss: 0.3676 - learning_rate: 0.0025 - gradient_norm: 3.4232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:13:56.791943: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 43s 110ms/step - det_loss: 0.3038 - cls_loss: 0.1901 - box_loss: 0.0023 - reg_l2_loss: 0.0636 - loss: 0.3674 - learning_rate: 0.0025 - gradient_norm: 3.4216 - val_det_loss: 0.2053 - val_cls_loss: 0.1442 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0636 - val_loss: 0.2689\n",
      "Epoch 3/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.2943 - cls_loss: 0.1963 - box_loss: 0.0020 - reg_l2_loss: 0.0636 - loss: 0.3579 - learning_rate: 0.0025 - gradient_norm: 3.4136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:14:40.221775: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 43s 110ms/step - det_loss: 0.2940 - cls_loss: 0.1961 - box_loss: 0.0020 - reg_l2_loss: 0.0636 - loss: 0.3576 - learning_rate: 0.0025 - gradient_norm: 3.4105 - val_det_loss: 0.1498 - val_cls_loss: 0.1059 - val_box_loss: 8.7799e-04 - val_reg_l2_loss: 0.0636 - val_loss: 0.2134\n",
      "Epoch 4/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.2461 - cls_loss: 0.1641 - box_loss: 0.0016 - reg_l2_loss: 0.0637 - loss: 0.3098 - learning_rate: 0.0025 - gradient_norm: 3.3304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:15:23.538346: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 43s 110ms/step - det_loss: 0.2461 - cls_loss: 0.1641 - box_loss: 0.0016 - reg_l2_loss: 0.0637 - loss: 0.3097 - learning_rate: 0.0025 - gradient_norm: 3.3281 - val_det_loss: 0.1981 - val_cls_loss: 0.1481 - val_box_loss: 9.9877e-04 - val_reg_l2_loss: 0.0637 - val_loss: 0.2617\n",
      "Epoch 5/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.2250 - cls_loss: 0.1597 - box_loss: 0.0013 - reg_l2_loss: 0.0637 - loss: 0.2887 - learning_rate: 0.0024 - gradient_norm: 3.2109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:16:06.969814: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 47s 120ms/step - det_loss: 0.2248 - cls_loss: 0.1596 - box_loss: 0.0013 - reg_l2_loss: 0.0637 - loss: 0.2884 - learning_rate: 0.0024 - gradient_norm: 3.2063 - val_det_loss: 0.1594 - val_cls_loss: 0.1218 - val_box_loss: 7.5144e-04 - val_reg_l2_loss: 0.0637 - val_loss: 0.2231\n",
      "Epoch 6/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.2282 - cls_loss: 0.1476 - box_loss: 0.0016 - reg_l2_loss: 0.0637 - loss: 0.2919 - learning_rate: 0.0024 - gradient_norm: 3.3590"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:16:54.712418: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 44s 111ms/step - det_loss: 0.2281 - cls_loss: 0.1475 - box_loss: 0.0016 - reg_l2_loss: 0.0637 - loss: 0.2918 - learning_rate: 0.0024 - gradient_norm: 3.3625 - val_det_loss: 0.2165 - val_cls_loss: 0.1500 - val_box_loss: 0.0013 - val_reg_l2_loss: 0.0637 - val_loss: 0.2802\n",
      "Epoch 7/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.2097 - cls_loss: 0.1414 - box_loss: 0.0014 - reg_l2_loss: 0.0637 - loss: 0.2735 - learning_rate: 0.0024 - gradient_norm: 3.1943"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:17:38.388079: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 44s 111ms/step - det_loss: 0.2095 - cls_loss: 0.1413 - box_loss: 0.0014 - reg_l2_loss: 0.0637 - loss: 0.2733 - learning_rate: 0.0024 - gradient_norm: 3.1904 - val_det_loss: 0.1273 - val_cls_loss: 0.1079 - val_box_loss: 3.8944e-04 - val_reg_l2_loss: 0.0637 - val_loss: 0.1911\n",
      "Epoch 8/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.1899 - cls_loss: 0.1277 - box_loss: 0.0012 - reg_l2_loss: 0.0637 - loss: 0.2536 - learning_rate: 0.0024 - gradient_norm: 2.9791"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:18:21.605445: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 43s 110ms/step - det_loss: 0.1899 - cls_loss: 0.1277 - box_loss: 0.0012 - reg_l2_loss: 0.0637 - loss: 0.2536 - learning_rate: 0.0024 - gradient_norm: 2.9782 - val_det_loss: 0.1306 - val_cls_loss: 0.1012 - val_box_loss: 5.8702e-04 - val_reg_l2_loss: 0.0637 - val_loss: 0.1943\n",
      "Epoch 9/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.1803 - cls_loss: 0.1225 - box_loss: 0.0012 - reg_l2_loss: 0.0637 - loss: 0.2441 - learning_rate: 0.0023 - gradient_norm: 2.8765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:19:05.341014: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 44s 111ms/step - det_loss: 0.1802 - cls_loss: 0.1225 - box_loss: 0.0012 - reg_l2_loss: 0.0637 - loss: 0.2439 - learning_rate: 0.0023 - gradient_norm: 2.8764 - val_det_loss: 0.1278 - val_cls_loss: 0.0933 - val_box_loss: 6.9079e-04 - val_reg_l2_loss: 0.0637 - val_loss: 0.1916\n",
      "Epoch 10/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.2143 - cls_loss: 0.1605 - box_loss: 0.0011 - reg_l2_loss: 0.0637 - loss: 0.2781 - learning_rate: 0.0023 - gradient_norm: 2.9668"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:19:48.647285: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 46s 117ms/step - det_loss: 0.2141 - cls_loss: 0.1604 - box_loss: 0.0011 - reg_l2_loss: 0.0637 - loss: 0.2778 - learning_rate: 0.0023 - gradient_norm: 2.9638 - val_det_loss: 0.1139 - val_cls_loss: 0.0949 - val_box_loss: 3.7983e-04 - val_reg_l2_loss: 0.0637 - val_loss: 0.1776\n",
      "Epoch 11/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.1819 - cls_loss: 0.1213 - box_loss: 0.0012 - reg_l2_loss: 0.0637 - loss: 0.2457 - learning_rate: 0.0022 - gradient_norm: 2.8739"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:20:35.248401: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 44s 111ms/step - det_loss: 0.1817 - cls_loss: 0.1212 - box_loss: 0.0012 - reg_l2_loss: 0.0637 - loss: 0.2454 - learning_rate: 0.0022 - gradient_norm: 2.8697 - val_det_loss: 0.1399 - val_cls_loss: 0.1190 - val_box_loss: 4.1674e-04 - val_reg_l2_loss: 0.0637 - val_loss: 0.2036\n",
      "Epoch 12/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.1757 - cls_loss: 0.1199 - box_loss: 0.0011 - reg_l2_loss: 0.0637 - loss: 0.2394 - learning_rate: 0.0022 - gradient_norm: 2.8210"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:21:18.586661: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 43s 110ms/step - det_loss: 0.1756 - cls_loss: 0.1198 - box_loss: 0.0011 - reg_l2_loss: 0.0637 - loss: 0.2393 - learning_rate: 0.0022 - gradient_norm: 2.8179 - val_det_loss: 0.1113 - val_cls_loss: 0.0940 - val_box_loss: 3.4706e-04 - val_reg_l2_loss: 0.0637 - val_loss: 0.1751\n",
      "Epoch 13/50\n",
      "394/394 [==============================] - ETA: 0s - det_loss: 0.1743 - cls_loss: 0.1177 - box_loss: 0.0011 - reg_l2_loss: 0.0637 - loss: 0.2381 - learning_rate: 0.0021 - gradient_norm: 2.8943"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:22:02.194498: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 44s 111ms/step - det_loss: 0.1742 - cls_loss: 0.1176 - box_loss: 0.0011 - reg_l2_loss: 0.0637 - loss: 0.2380 - learning_rate: 0.0021 - gradient_norm: 2.8916 - val_det_loss: 0.1275 - val_cls_loss: 0.1092 - val_box_loss: 3.6664e-04 - val_reg_l2_loss: 0.0637 - val_loss: 0.1913\n",
      "Epoch 14/50\n",
      "271/394 [===================>..........] - ETA: 13s - det_loss: 0.1643 - cls_loss: 0.1163 - box_loss: 9.6085e-04 - reg_l2_loss: 0.0637 - loss: 0.2281 - learning_rate: 0.0021 - gradient_norm: 2.8195"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m spec0 \u001b[38;5;241m=\u001b[39m model_spec\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mefficientdet_lite0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mobject_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspec0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_whole_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py:260\u001b[0m, in \u001b[0;36mObjectDetector.create\u001b[0;34m(cls, train_data, model_spec, validation_data, epochs, batch_size, train_whole_model, do_train)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_train:\n\u001b[1;32m    259\u001b[0m   tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetraining the models...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 260\u001b[0m   \u001b[43mobject_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m   object_detector\u001b[38;5;241m.\u001b[39mcreate_model()\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py:123\u001b[0m, in \u001b[0;36mObjectDetector.train\u001b[0;34m(self, train_data, validation_data, epochs, batch_size)\u001b[0m\n\u001b[1;32m    119\u001b[0m train_ds, steps_per_epoch, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_and_steps(\n\u001b[1;32m    120\u001b[0m     train_data, batch_size, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m validation_ds, validation_steps, val_json_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_and_steps(\n\u001b[1;32m    122\u001b[0m     validation_data, batch_size, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalidation_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_json_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py:265\u001b[0m, in \u001b[0;36mEfficientDetModelSpec.train\u001b[0;34m(self, model, train_dataset, steps_per_epoch, val_dataset, validation_steps, epochs, batch_size, val_json_file)\u001b[0m\n\u001b[1;32m    263\u001b[0m train\u001b[38;5;241m.\u001b[39msetup_model(model, config)\n\u001b[1;32m    264\u001b[0m train\u001b[38;5;241m.\u001b[39minit_experimental(config)\n\u001b[0;32m--> 265\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spec0 = model_spec.get('efficientdet_lite0')\n",
    "model = object_detector.create(train_data, model_spec=spec0, batch_size=2, train_whole_model=True, validation_data=validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bd67e41-805c-4a80-96aa-f0edb1082d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Values = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cff90d2-5d05-41a9-8356-a955d60c3e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AP': 0.9186616, 'AP50': 1.0, 'AP75': 1.0, 'APs': -1.0, 'APm': 0.9141914, 'APl': 0.9245618, 'ARmax1': 0.92583334, 'ARmax10': 0.9546667, 'ARmax100': 0.9546667, 'ARs': -1.0, 'ARm': 0.915, 'ARl': 0.9628146, 'AP_/teamprop_red': 0.88633716, 'AP_/teamprop_blue': 0.950986}\n"
     ]
    }
   ],
   "source": [
    "print(Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62738479-d2f7-40fd-adcf-6370b08bb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec4 = model_spec.get('efficientdet_lite4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df6c42ac-7c33-444b-bb46-0bc29787ed59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c707bf21-f71f-427e-865a-0ebb87c00588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set at program startup\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc3b1e83-172a-4de5-b883-268205c7d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d7021fc-b804-4234-9dfc-b88a801c50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec4 = model_spec.get('efficientdet_lite1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "284e3abd-9363-48d9-b9e1-0fdf24f55ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:47:12.087322: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8801\n",
      "2024-01-04 18:47:12.814098: W tensorflow/stream_executor/gpu/asm_compiler.cc:230] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2024-01-04 18:47:12.814134: W tensorflow/stream_executor/gpu/asm_compiler.cc:233] Used ptxas at ptxas\n",
      "2024-01-04 18:47:12.814158: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 20s 214ms/step - det_loss: 1.3681 - cls_loss: 0.9662 - box_loss: 0.0080 - reg_l2_loss: 0.0706 - loss: 1.4387 - learning_rate: 0.0115 - gradient_norm: 1.8255 - val_det_loss: 1.0233 - val_cls_loss: 0.8133 - val_box_loss: 0.0042 - val_reg_l2_loss: 0.0706 - val_loss: 1.0939\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 5s 181ms/step - det_loss: 0.4981 - cls_loss: 0.3532 - box_loss: 0.0029 - reg_l2_loss: 0.0706 - loss: 0.5688 - learning_rate: 0.0150 - gradient_norm: 1.8802 - val_det_loss: 0.3812 - val_cls_loss: 0.2686 - val_box_loss: 0.0023 - val_reg_l2_loss: 0.0706 - val_loss: 0.4519\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 5s 180ms/step - det_loss: 0.2676 - cls_loss: 0.1873 - box_loss: 0.0016 - reg_l2_loss: 0.0707 - loss: 0.3383 - learning_rate: 0.0149 - gradient_norm: 1.3890 - val_det_loss: 0.3123 - val_cls_loss: 0.2061 - val_box_loss: 0.0021 - val_reg_l2_loss: 0.0707 - val_loss: 0.3829\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 5s 189ms/step - det_loss: 0.2160 - cls_loss: 0.1485 - box_loss: 0.0014 - reg_l2_loss: 0.0707 - loss: 0.2867 - learning_rate: 0.0148 - gradient_norm: 1.5539 - val_det_loss: 0.2278 - val_cls_loss: 0.1417 - val_box_loss: 0.0017 - val_reg_l2_loss: 0.0707 - val_loss: 0.2985\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 7s 252ms/step - det_loss: 0.1912 - cls_loss: 0.1330 - box_loss: 0.0012 - reg_l2_loss: 0.0707 - loss: 0.2619 - learning_rate: 0.0147 - gradient_norm: 1.3820 - val_det_loss: 0.1996 - val_cls_loss: 0.1314 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.0707 - val_loss: 0.2703\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 5s 182ms/step - det_loss: 0.1710 - cls_loss: 0.1199 - box_loss: 0.0010 - reg_l2_loss: 0.0707 - loss: 0.2417 - learning_rate: 0.0145 - gradient_norm: 1.5148 - val_det_loss: 0.1739 - val_cls_loss: 0.1116 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0707 - val_loss: 0.2446\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 6s 192ms/step - det_loss: 0.1679 - cls_loss: 0.1159 - box_loss: 0.0010 - reg_l2_loss: 0.0707 - loss: 0.2386 - learning_rate: 0.0144 - gradient_norm: 1.3769 - val_det_loss: 0.1640 - val_cls_loss: 0.1092 - val_box_loss: 0.0011 - val_reg_l2_loss: 0.0707 - val_loss: 0.2347\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 5s 185ms/step - det_loss: 0.1464 - cls_loss: 0.1027 - box_loss: 8.7480e-04 - reg_l2_loss: 0.0707 - loss: 0.2171 - learning_rate: 0.0141 - gradient_norm: 1.2114 - val_det_loss: 0.1416 - val_cls_loss: 0.0939 - val_box_loss: 9.5285e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.2123\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 5s 183ms/step - det_loss: 0.1443 - cls_loss: 0.1020 - box_loss: 8.4690e-04 - reg_l2_loss: 0.0707 - loss: 0.2150 - learning_rate: 0.0139 - gradient_norm: 1.7953 - val_det_loss: 0.1667 - val_cls_loss: 0.1010 - val_box_loss: 0.0013 - val_reg_l2_loss: 0.0707 - val_loss: 0.2374\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 6s 202ms/step - det_loss: 0.1323 - cls_loss: 0.0914 - box_loss: 8.1791e-04 - reg_l2_loss: 0.0707 - loss: 0.2031 - learning_rate: 0.0137 - gradient_norm: 1.2950 - val_det_loss: 0.1717 - val_cls_loss: 0.1263 - val_box_loss: 9.0860e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.2424\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 6s 191ms/step - det_loss: 0.1331 - cls_loss: 0.0940 - box_loss: 7.8208e-04 - reg_l2_loss: 0.0707 - loss: 0.2039 - learning_rate: 0.0134 - gradient_norm: 1.4326 - val_det_loss: 0.1277 - val_cls_loss: 0.0892 - val_box_loss: 7.7009e-04 - val_reg_l2_loss: 0.0708 - val_loss: 0.1984\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 5s 180ms/step - det_loss: 0.1237 - cls_loss: 0.0853 - box_loss: 7.6756e-04 - reg_l2_loss: 0.0708 - loss: 0.1944 - learning_rate: 0.0131 - gradient_norm: 1.3627 - val_det_loss: 0.1100 - val_cls_loss: 0.0742 - val_box_loss: 7.1472e-04 - val_reg_l2_loss: 0.0708 - val_loss: 0.1807\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 5s 181ms/step - det_loss: 0.1239 - cls_loss: 0.0896 - box_loss: 6.8512e-04 - reg_l2_loss: 0.0707 - loss: 0.1946 - learning_rate: 0.0127 - gradient_norm: 1.0991 - val_det_loss: 0.1165 - val_cls_loss: 0.0830 - val_box_loss: 6.7076e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1873\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 5s 179ms/step - det_loss: 0.1202 - cls_loss: 0.0862 - box_loss: 6.7828e-04 - reg_l2_loss: 0.0707 - loss: 0.1909 - learning_rate: 0.0124 - gradient_norm: 1.1100 - val_det_loss: 0.1072 - val_cls_loss: 0.0711 - val_box_loss: 7.2179e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1779\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 6s 207ms/step - det_loss: 0.1111 - cls_loss: 0.0800 - box_loss: 6.2358e-04 - reg_l2_loss: 0.0707 - loss: 0.1819 - learning_rate: 0.0120 - gradient_norm: 1.0451 - val_det_loss: 0.1088 - val_cls_loss: 0.0733 - val_box_loss: 7.0960e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1795\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 5s 181ms/step - det_loss: 0.0959 - cls_loss: 0.0704 - box_loss: 5.0974e-04 - reg_l2_loss: 0.0707 - loss: 0.1666 - learning_rate: 0.0116 - gradient_norm: 0.9472 - val_det_loss: 0.0923 - val_cls_loss: 0.0663 - val_box_loss: 5.2034e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1630\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 5s 184ms/step - det_loss: 0.0983 - cls_loss: 0.0718 - box_loss: 5.3035e-04 - reg_l2_loss: 0.0707 - loss: 0.1691 - learning_rate: 0.0112 - gradient_norm: 1.0284 - val_det_loss: 0.0927 - val_cls_loss: 0.0708 - val_box_loss: 4.3758e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1634\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 5s 179ms/step - det_loss: 0.0964 - cls_loss: 0.0703 - box_loss: 5.2198e-04 - reg_l2_loss: 0.0707 - loss: 0.1672 - learning_rate: 0.0108 - gradient_norm: 0.9373 - val_det_loss: 0.0938 - val_cls_loss: 0.0689 - val_box_loss: 4.9702e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1645\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 5s 190ms/step - det_loss: 0.0985 - cls_loss: 0.0722 - box_loss: 5.2663e-04 - reg_l2_loss: 0.0707 - loss: 0.1692 - learning_rate: 0.0103 - gradient_norm: 1.0649 - val_det_loss: 0.0906 - val_cls_loss: 0.0661 - val_box_loss: 4.8985e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1613\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 6s 199ms/step - det_loss: 0.0920 - cls_loss: 0.0691 - box_loss: 4.5892e-04 - reg_l2_loss: 0.0707 - loss: 0.1627 - learning_rate: 0.0099 - gradient_norm: 1.0693 - val_det_loss: 0.0893 - val_cls_loss: 0.0646 - val_box_loss: 4.9349e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1599\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 5s 180ms/step - det_loss: 0.0950 - cls_loss: 0.0702 - box_loss: 4.9617e-04 - reg_l2_loss: 0.0707 - loss: 0.1657 - learning_rate: 0.0094 - gradient_norm: 1.0532 - val_det_loss: 0.0822 - val_cls_loss: 0.0613 - val_box_loss: 4.1794e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1529\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 5s 181ms/step - det_loss: 0.0917 - cls_loss: 0.0657 - box_loss: 5.1984e-04 - reg_l2_loss: 0.0707 - loss: 0.1624 - learning_rate: 0.0089 - gradient_norm: 1.0481 - val_det_loss: 0.0846 - val_cls_loss: 0.0597 - val_box_loss: 4.9768e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1553\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 5s 188ms/step - det_loss: 0.0947 - cls_loss: 0.0708 - box_loss: 4.7761e-04 - reg_l2_loss: 0.0707 - loss: 0.1654 - learning_rate: 0.0085 - gradient_norm: 1.1944 - val_det_loss: 0.0920 - val_cls_loss: 0.0693 - val_box_loss: 4.5359e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1627\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 5s 183ms/step - det_loss: 0.0914 - cls_loss: 0.0672 - box_loss: 4.8436e-04 - reg_l2_loss: 0.0707 - loss: 0.1621 - learning_rate: 0.0080 - gradient_norm: 0.9348 - val_det_loss: 0.0821 - val_cls_loss: 0.0614 - val_box_loss: 4.1419e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1528\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 6s 200ms/step - det_loss: 0.0887 - cls_loss: 0.0665 - box_loss: 4.4343e-04 - reg_l2_loss: 0.0707 - loss: 0.1593 - learning_rate: 0.0075 - gradient_norm: 1.0624 - val_det_loss: 0.0947 - val_cls_loss: 0.0714 - val_box_loss: 4.6720e-04 - val_reg_l2_loss: 0.0707 - val_loss: 0.1654\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 5s 180ms/step - det_loss: 0.0897 - cls_loss: 0.0667 - box_loss: 4.6027e-04 - reg_l2_loss: 0.0707 - loss: 0.1603 - learning_rate: 0.0070 - gradient_norm: 0.9708 - val_det_loss: 0.0802 - val_cls_loss: 0.0586 - val_box_loss: 4.3196e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1508\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 6s 192ms/step - det_loss: 0.0820 - cls_loss: 0.0633 - box_loss: 3.7540e-04 - reg_l2_loss: 0.0706 - loss: 0.1527 - learning_rate: 0.0065 - gradient_norm: 1.0151 - val_det_loss: 0.0817 - val_cls_loss: 0.0588 - val_box_loss: 4.5846e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1523\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 5s 184ms/step - det_loss: 0.0806 - cls_loss: 0.0620 - box_loss: 3.7311e-04 - reg_l2_loss: 0.0706 - loss: 0.1513 - learning_rate: 0.0061 - gradient_norm: 0.8810 - val_det_loss: 0.0840 - val_cls_loss: 0.0624 - val_box_loss: 4.3200e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1546\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 5s 181ms/step - det_loss: 0.0833 - cls_loss: 0.0624 - box_loss: 4.1875e-04 - reg_l2_loss: 0.0706 - loss: 0.1539 - learning_rate: 0.0056 - gradient_norm: 0.9715 - val_det_loss: 0.0943 - val_cls_loss: 0.0709 - val_box_loss: 4.6686e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1649\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 6s 208ms/step - det_loss: 0.0758 - cls_loss: 0.0568 - box_loss: 3.7884e-04 - reg_l2_loss: 0.0706 - loss: 0.1464 - learning_rate: 0.0051 - gradient_norm: 0.8873 - val_det_loss: 0.0844 - val_cls_loss: 0.0649 - val_box_loss: 3.9103e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1550\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 5s 184ms/step - det_loss: 0.0788 - cls_loss: 0.0585 - box_loss: 4.0736e-04 - reg_l2_loss: 0.0706 - loss: 0.1494 - learning_rate: 0.0047 - gradient_norm: 0.9160 - val_det_loss: 0.0893 - val_cls_loss: 0.0701 - val_box_loss: 3.8456e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1599\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 5s 179ms/step - det_loss: 0.0803 - cls_loss: 0.0620 - box_loss: 3.6450e-04 - reg_l2_loss: 0.0706 - loss: 0.1509 - learning_rate: 0.0042 - gradient_norm: 0.9221 - val_det_loss: 0.0793 - val_cls_loss: 0.0601 - val_box_loss: 3.8444e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1500\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 5s 183ms/step - det_loss: 0.0762 - cls_loss: 0.0576 - box_loss: 3.7255e-04 - reg_l2_loss: 0.0706 - loss: 0.1468 - learning_rate: 0.0038 - gradient_norm: 0.8544 - val_det_loss: 0.0777 - val_cls_loss: 0.0607 - val_box_loss: 3.3946e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1483\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 5s 188ms/step - det_loss: 0.0776 - cls_loss: 0.0593 - box_loss: 3.6606e-04 - reg_l2_loss: 0.0706 - loss: 0.1482 - learning_rate: 0.0034 - gradient_norm: 0.9333 - val_det_loss: 0.0772 - val_cls_loss: 0.0589 - val_box_loss: 3.6652e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1478\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 6s 197ms/step - det_loss: 0.0788 - cls_loss: 0.0602 - box_loss: 3.7191e-04 - reg_l2_loss: 0.0706 - loss: 0.1494 - learning_rate: 0.0030 - gradient_norm: 0.9409 - val_det_loss: 0.0721 - val_cls_loss: 0.0549 - val_box_loss: 3.4474e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1427\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 5s 181ms/step - det_loss: 0.0792 - cls_loss: 0.0616 - box_loss: 3.5275e-04 - reg_l2_loss: 0.0706 - loss: 0.1498 - learning_rate: 0.0026 - gradient_norm: 0.9544 - val_det_loss: 0.0737 - val_cls_loss: 0.0542 - val_box_loss: 3.9004e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1443\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 5s 182ms/step - det_loss: 0.0759 - cls_loss: 0.0581 - box_loss: 3.5621e-04 - reg_l2_loss: 0.0706 - loss: 0.1465 - learning_rate: 0.0023 - gradient_norm: 0.9118 - val_det_loss: 0.0772 - val_cls_loss: 0.0607 - val_box_loss: 3.3047e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1478\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 6s 192ms/step - det_loss: 0.0730 - cls_loss: 0.0564 - box_loss: 3.3054e-04 - reg_l2_loss: 0.0706 - loss: 0.1436 - learning_rate: 0.0019 - gradient_norm: 0.7937 - val_det_loss: 0.0712 - val_cls_loss: 0.0540 - val_box_loss: 3.4484e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1418\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 5s 183ms/step - det_loss: 0.0742 - cls_loss: 0.0577 - box_loss: 3.3083e-04 - reg_l2_loss: 0.0706 - loss: 0.1448 - learning_rate: 0.0016 - gradient_norm: 0.8119 - val_det_loss: 0.0712 - val_cls_loss: 0.0539 - val_box_loss: 3.4559e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1418\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 6s 198ms/step - det_loss: 0.0731 - cls_loss: 0.0561 - box_loss: 3.3993e-04 - reg_l2_loss: 0.0706 - loss: 0.1437 - learning_rate: 0.0014 - gradient_norm: 0.8145 - val_det_loss: 0.0670 - val_cls_loss: 0.0507 - val_box_loss: 3.2459e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1375\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 5s 181ms/step - det_loss: 0.0709 - cls_loss: 0.0558 - box_loss: 3.0142e-04 - reg_l2_loss: 0.0706 - loss: 0.1415 - learning_rate: 0.0011 - gradient_norm: 0.8304 - val_det_loss: 0.0673 - val_cls_loss: 0.0515 - val_box_loss: 3.1634e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1379\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 6s 193ms/step - det_loss: 0.0719 - cls_loss: 0.0549 - box_loss: 3.3969e-04 - reg_l2_loss: 0.0706 - loss: 0.1425 - learning_rate: 8.5196e-04 - gradient_norm: 0.8004 - val_det_loss: 0.0711 - val_cls_loss: 0.0552 - val_box_loss: 3.1732e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1416\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 5s 181ms/step - det_loss: 0.0732 - cls_loss: 0.0573 - box_loss: 3.1781e-04 - reg_l2_loss: 0.0706 - loss: 0.1438 - learning_rate: 6.4337e-04 - gradient_norm: 0.8331 - val_det_loss: 0.0672 - val_cls_loss: 0.0510 - val_box_loss: 3.2313e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1378\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 5s 179ms/step - det_loss: 0.0689 - cls_loss: 0.0547 - box_loss: 2.8389e-04 - reg_l2_loss: 0.0706 - loss: 0.1395 - learning_rate: 4.6295e-04 - gradient_norm: 0.7934 - val_det_loss: 0.0697 - val_cls_loss: 0.0531 - val_box_loss: 3.3158e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1403\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 6s 199ms/step - det_loss: 0.0714 - cls_loss: 0.0565 - box_loss: 2.9910e-04 - reg_l2_loss: 0.0706 - loss: 0.1420 - learning_rate: 3.1145e-04 - gradient_norm: 0.8066 - val_det_loss: 0.0668 - val_cls_loss: 0.0509 - val_box_loss: 3.1738e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1374\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 5s 190ms/step - det_loss: 0.0742 - cls_loss: 0.0575 - box_loss: 3.3507e-04 - reg_l2_loss: 0.0706 - loss: 0.1448 - learning_rate: 1.8949e-04 - gradient_norm: 0.8424 - val_det_loss: 0.0663 - val_cls_loss: 0.0505 - val_box_loss: 3.1447e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1369\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 5s 179ms/step - det_loss: 0.0706 - cls_loss: 0.0561 - box_loss: 2.9084e-04 - reg_l2_loss: 0.0706 - loss: 0.1412 - learning_rate: 9.7563e-05 - gradient_norm: 0.7922 - val_det_loss: 0.0661 - val_cls_loss: 0.0504 - val_box_loss: 3.1336e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1367\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 5s 183ms/step - det_loss: 0.0670 - cls_loss: 0.0528 - box_loss: 2.8438e-04 - reg_l2_loss: 0.0706 - loss: 0.1376 - learning_rate: 3.6059e-05 - gradient_norm: 0.7689 - val_det_loss: 0.0659 - val_cls_loss: 0.0502 - val_box_loss: 3.1315e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1365\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 5s 181ms/step - det_loss: 0.0702 - cls_loss: 0.0551 - box_loss: 3.0070e-04 - reg_l2_loss: 0.0706 - loss: 0.1408 - learning_rate: 5.2263e-06 - gradient_norm: 0.7319 - val_det_loss: 0.0658 - val_cls_loss: 0.0502 - val_box_loss: 3.1290e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1364\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 6s 210ms/step - det_loss: 0.0703 - cls_loss: 0.0560 - box_loss: 2.8639e-04 - reg_l2_loss: 0.0706 - loss: 0.1409 - learning_rate: 5.1910e-06 - gradient_norm: 0.8074 - val_det_loss: 0.0657 - val_cls_loss: 0.0501 - val_box_loss: 3.1276e-04 - val_reg_l2_loss: 0.0706 - val_loss: 0.1363\n"
     ]
    }
   ],
   "source": [
    "model2 = object_detector.create(train_data, model_spec=spec4, batch_size=12, epochs=50, train_whole_model=True, validation_data=validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7abe717e-5beb-4c95-83d3-4e448247718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:51:47.776763: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2024-01-04 18:51:58.450380: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.\n",
      "2024-01-04 18:52:01.592993: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2024-01-04 18:52:01.593014: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2024-01-04 18:52:01.593592: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmp011nx5tm\n",
      "2024-01-04 18:52:01.653132: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2024-01-04 18:52:01.653155: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmp011nx5tm\n",
      "2024-01-04 18:52:01.842280: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2024-01-04 18:52:02.739028: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmp011nx5tm\n",
      "2024-01-04 18:52:03.137410: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 1543818 microseconds.\n",
      "2024-01-04 18:52:03.921647: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-04 18:52:04.729788: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 3.547 G  ops, equivalently 1.773 G  MACs\n",
      "\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 0\n",
      "2024-01-04 18:53:17.959307: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 3.547 G  ops, equivalently 1.773 G  MACs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2.export(export_dir='.',     tflite_filename='model_efficientDet1.tflite',\n",
    "    label_filename='labels_efficientDet1.txt',\n",
    "    vocab_filename='vocab_efficientDet1.txt',\n",
    "    saved_model_filename='saved_model',\n",
    "    tfjs_folder_name='tfjs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ff0d7-4a0c-44c6-89cc-166bd0224d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec4 = model_spec.get('efficientdet_lite1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20b9a7b0-c428-4a1e-a43e-ae67e0156946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Values2 = model2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69686bcf-30e4-4873-9729-31f9d01dc7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AP': 0.9544427, 'AP50': 1.0, 'AP75': 1.0, 'APs': -1.0, 'APm': 0.91936195, 'APl': 0.96342546, 'ARmax1': 0.95883334, 'ARmax10': 0.9751667, 'ARmax100': 0.9751667, 'ARs': -1.0, 'ARm': 0.935, 'ARl': 0.9820366, 'AP_/teamprop_red': 0.9324126, 'AP_/teamprop_blue': 0.9764728}\n"
     ]
    }
   ],
   "source": [
    "print(Values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c3780-50b5-4445-814a-c6bc336e9674",
   "metadata": {},
   "source": [
    "# visualise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f08ee22-0253-405b-8978-60446f388bb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the labels into a list\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m classes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m???\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mmodel_spec\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_classes\n\u001b[1;32m      9\u001b[0m label_map \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel_spec\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlabel_map\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "model_path = 'model.tflite'\n",
    "\n",
    "# Load the labels into a list\n",
    "classes = ['???'] * model.model_spec.config.num_classes\n",
    "label_map = model.model_spec.config.label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af169b3d-a992-4208-adc7-5358b79c7872",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_id, label_name in label_map.as_dict().items():\n",
    "  classes[label_id-1] = label_name\n",
    "\n",
    "# Define a list of colors for visualization\n",
    "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "  original_image = img\n",
    "  resized_img = tf.image.resize(img, input_size)\n",
    "  resized_img = resized_img[tf.newaxis, :]\n",
    "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
    "  return resized_img, original_image\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "\n",
    "  signature_fn = interpreter.get_signature_runner()\n",
    "\n",
    "  # Feed the input image to the model\n",
    "  output = signature_fn(images=image)\n",
    "\n",
    "  # Get all outputs from the model\n",
    "  count = int(np.squeeze(output['output_0']))\n",
    "  scores = np.squeeze(output['output_1'])\n",
    "  classes = np.squeeze(output['output_2'])\n",
    "  boxes = np.squeeze(output['output_3'])\n",
    "\n",
    "  results = []\n",
    "  for i in range(count):\n",
    "    if scores[i] >= threshold:\n",
    "      result = {\n",
    "        'bounding_box': boxes[i],\n",
    "        'class_id': classes[i],\n",
    "        'score': scores[i]\n",
    "      }\n",
    "      results.append(result)\n",
    "  return results\n",
    "\n",
    "\n",
    "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
    "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "  # Load the input shape required by the model\n",
    "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
    "\n",
    "  # Load the input image and preprocess it\n",
    "  preprocessed_image, original_image = preprocess_image(\n",
    "      image_path,\n",
    "      (input_height, input_width)\n",
    "    )\n",
    "\n",
    "  # Run object detection on the input image\n",
    "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "\n",
    "  # Plot the detection results on the input image\n",
    "  original_image_np = original_image.numpy().astype(np.uint8)\n",
    "  for obj in results:\n",
    "    # Convert the object bounding box from relative coordinates to absolute\n",
    "    # coordinates based on the original image resolution\n",
    "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "    xmin = int(xmin * original_image_np.shape[1])\n",
    "    xmax = int(xmax * original_image_np.shape[1])\n",
    "    ymin = int(ymin * original_image_np.shape[0])\n",
    "    ymax = int(ymax * original_image_np.shape[0])\n",
    "\n",
    "    # Find the class index of the current object\n",
    "    class_id = int(obj['class_id'])\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    color = [int(c) for c in COLORS[class_id]]\n",
    "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "    # Make adjustments to make the label visible for all objects\n",
    "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
    "    cv2.putText(original_image_np, label, (xmin, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "  # Return the final image\n",
    "  original_uint8 = original_image_np.astype(np.uint8)\n",
    "  return original_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0948df-dce2-4065-ae01-79664ab36358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run object detection and show the detection results\n",
    "\n",
    "INPUT_IMAGE_URL = \"https://storage.googleapis.com/cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg\" #@param {type:\"string\"}\n",
    "DETECTION_THRESHOLD = 0.3 #@param {type:\"number\"}\n",
    "\n",
    "TEMP_FILE = '/tmp/image.png'\n",
    "\n",
    "im = Image.open(TEMP_FILE)\n",
    "im.thumbnail((512, 512), Image.ANTIALIAS)\n",
    "im.save(TEMP_FILE, 'PNG')\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Run inference and draw detection result on the local copy of the original file\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    TEMP_FILE,\n",
    "    interpreter,\n",
    "    threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "\n",
    "# Show the detection result\n",
    "Image.fromarray(detection_result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce21383-53ba-4a1c-a0b2-03e87c0829bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.lite.Interpreter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc6c61-a52f-442e-a387-5fd597a3177f",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed185449-ccd9-4528-a0b5-906daafde297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.9186616,\n",
       " 'AP50': 1.0,\n",
       " 'AP75': 1.0,\n",
       " 'APs': -1.0,\n",
       " 'APm': 0.9141914,\n",
       " 'APl': 0.9245618,\n",
       " 'ARmax1': 0.92583334,\n",
       " 'ARmax10': 0.9546667,\n",
       " 'ARmax100': 0.9546667,\n",
       " 'ARs': -1.0,\n",
       " 'ARm': 0.915,\n",
       " 'ARl': 0.9628146,\n",
       " 'AP_/teamprop_red': 0.88633716,\n",
       " 'AP_/teamprop_blue': 0.950986}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bb16ab7-0b51-4d9c-bc5f-12557113957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 17:13:35.849422: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2024-01-04 17:13:44.630864: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.\n",
      "2024-01-04 17:13:47.347961: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2024-01-04 17:13:47.347983: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2024-01-04 17:13:47.348414: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpwaq4uur4\n",
      "2024-01-04 17:13:47.395477: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2024-01-04 17:13:47.395496: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpwaq4uur4\n",
      "2024-01-04 17:13:47.550366: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2024-01-04 17:13:48.270794: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpwaq4uur4\n",
      "2024-01-04 17:13:48.606133: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 1257719 microseconds.\n",
      "2024-01-04 17:13:49.242196: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-04 17:13:49.865451: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 1.752 G  ops, equivalently 0.876 G  MACs\n",
      "\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 0\n",
      "2024-01-04 17:14:25.888760: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 1.752 G  ops, equivalently 0.876 G  MACs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.export(export_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c45211b-c0de-4892-b10e-a6f159b9a072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:04:36.785506: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2024-01-04 18:04:52.149373: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.\n",
      "2024-01-04 18:04:57.466311: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2024-01-04 18:04:57.466335: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2024-01-04 18:04:57.468407: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpmuxpr_pn\n",
      "2024-01-04 18:04:57.553791: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2024-01-04 18:04:57.553820: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpmuxpr_pn\n",
      "2024-01-04 18:04:57.828407: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2024-01-04 18:04:59.360659: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpmuxpr_pn\n",
      "2024-01-04 18:05:00.026810: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 2558504 microseconds.\n",
      "2024-01-04 18:05:01.249932: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-04 18:05:02.562912: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 37.576 G  ops, equivalently 18.788 G  MACs\n",
      "\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 0\n",
      "2024-01-04 18:13:25.656690: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 37.576 G  ops, equivalently 18.788 G  MACs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2.export(export_dir='.',     tflite_filename='model_efficientDet4.tflite',\n",
    "    label_filename='labels_efficientDet4.txt',\n",
    "    vocab_filename='vocab_efficientDet4.txt',\n",
    "    saved_model_filename='saved_model',\n",
    "    tfjs_folder_name='tfjs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a21cb93f-cb91-4222-9cae-74b3b72b1f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 43s 876ms/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.9082702,\n",
       " 'AP50': 1.0,\n",
       " 'AP75': 1.0,\n",
       " 'APs': -1.0,\n",
       " 'APm': 0.8655941,\n",
       " 'APl': 0.91969395,\n",
       " 'ARmax1': 0.92583334,\n",
       " 'ARmax10': 0.92583334,\n",
       " 'ARmax100': 0.92583334,\n",
       " 'ARs': -1.0,\n",
       " 'ARm': 0.87,\n",
       " 'ARl': 0.93524027,\n",
       " 'AP_/teamprop_red': 0.87334126,\n",
       " 'AP_/teamprop_blue': 0.94319904}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_tflite('model.tflite', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3acfe8c6-3ba4-42f4-adfb-97a987665e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/49 [>.............................] - ETA: 14:16"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_tflite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_efficientDet4.tflite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py:155\u001b[0m, in \u001b[0;36mObjectDetector.evaluate_tflite\u001b[0;34m(self, tflite_filepath, data)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the TFLite model.\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m ds \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgen_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_spec, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_tflite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtflite_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotations_json_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py:377\u001b[0m, in \u001b[0;36mEfficientDetModelSpec.evaluate_tflite\u001b[0;34m(self, tflite_filepath, dataset, steps, json_file)\u001b[0m\n\u001b[1;32m    374\u001b[0m progbar \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mProgbar(steps)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[1;32m    376\u001b[0m   \u001b[38;5;66;03m# Get the output result after post-processing NMS op.\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m   _, nms_scores, nms_classes, nms_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mlite_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m   \u001b[38;5;66;03m# CLASS_OFFSET is used since label_id for `background` is 0 in label_map\u001b[39;00m\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;66;03m# while it's not actually included the model. We don't need to add the\u001b[39;00m\n\u001b[1;32m    381\u001b[0m   \u001b[38;5;66;03m# offset in the Android application.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m   nms_classes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m postprocess\u001b[38;5;241m.\u001b[39mCLASS_OFFSET\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/third_party/efficientdet/keras/eval_tflite.py:93\u001b[0m, in \u001b[0;36mLiteRunner.run\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     90\u001b[0m   image \u001b[38;5;241m=\u001b[39m image \u001b[38;5;241m/\u001b[39m scale \u001b[38;5;241m+\u001b[39m zero_point\n\u001b[1;32m     91\u001b[0m   image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image, dtype\u001b[38;5;241m=\u001b[39minput_detail[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 93\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msignature_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_output\u001b[39m(idx):\n\u001b[1;32m     96\u001b[0m   output_detail \u001b[38;5;241m=\u001b[39m output_details[idx]\n",
      "File \u001b[0;32m~/Desktop/R2045/CenterStage-TensorflowLite/ftc_center_stage_env/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py:258\u001b[0m, in \u001b[0;36mSignatureRunner.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_name, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpreter_wrapper\u001b[38;5;241m.\u001b[39mSetTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs[input_name], value,\n\u001b[1;32m    256\u001b[0m                                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subgraph_index)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_subgraph_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output_name, output_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2.evaluate_tflite('model_efficientDet4.tflite', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b088a6c8-73b0-4797-95fe-2d167315b573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
